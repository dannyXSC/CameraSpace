{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from multiprocessing.managers import SharedMemoryManager\n",
    "import click\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import dill\n",
    "import hydra\n",
    "import pathlib\n",
    "import skvideo.io\n",
    "from omegaconf import OmegaConf\n",
    "import scipy.spatial.transform as st\n",
    "from diffusion_policy.common.precise_sleep import precise_wait\n",
    "from diffusion_policy.real_world.real_inference_util import (\n",
    "    get_real_obs_resolution,\n",
    "    get_real_obs_dict,\n",
    ")\n",
    "from diffusion_policy.common.pytorch_util import dict_apply\n",
    "from diffusion_policy.workspace.base_workspace import BaseWorkspace\n",
    "from diffusion_policy.policy.base_image_policy import BaseImagePolicy\n",
    "from diffusion_policy.common.cv2_util import get_image_transform\n",
    "\n",
    "from diffusion_policy.dataset.h2r_dataset import H2RDataset\n",
    "\n",
    "OmegaConf.register_new_resolver(\"eval\", eval, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"data/outputs/2025.04.12/08.22.59_train_diffusion_unet_hybrid_square_image/checkpoints/latest.ckpt\"\n",
    "payload = torch.load(open(ckpt_path, \"rb\"), pickle_module=dill)\n",
    "cfg = payload[\"cfg\"]\n",
    "cls = hydra.utils.get_class(cfg._target_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'shape_meta': {'obs': {'human': {'shape': [3, 240, 426], 'type': 'rgb'}, 'robot0_eef_pos': {'shape': [3]}, 'robot0_eef_rot_pos': {'shape': [3]}, 'robot0_gripper_qpos': {'shape': [1]}}, 'action': {'shape': [10]}}, 'dataset_path_list': ['/home/ubuntu/project/Moore-AnimateAnyone/data/writing/writing_cvpr/episode_0.hdf5'], 'base_dataset_path': '/data1/dataset', 'horizon': 4, 'pad_before': 3, 'pad_after': 3, 'n_obs_steps': 4, 'abs_action': True, 'rotation_rep': 'rotation_6d', 'use_legacy_normalizer': False, 'use_cache': True, 'seed': 42, 'val_ratio': 0.02, 'is_task': False}\n",
      "Acquiring lock on cache.\n",
      "Loading cached ReplayBuffer from Disk.\n",
      "action\n",
      "human\n",
      "robot\n",
      "robot0_eef_pos\n",
      "robot0_eef_rot_pos\n",
      "robot0_gripper_qpos\n",
      "Loaded!\n"
     ]
    }
   ],
   "source": [
    "task_cfg = cfg.task.dataset\n",
    "\n",
    "new_cfg = {**task_cfg}\n",
    "new_cfg[\"dataset_path_list\"] = [\n",
    "    \"/home/ubuntu/project/Moore-AnimateAnyone/data/writing/writing_cvpr/episode_0.hdf5\"\n",
    "]\n",
    "new_cfg[\"is_task\"] = False\n",
    "new_cfg.pop(\"_target_\")\n",
    "print(new_cfg)\n",
    "\n",
    "a = dict()\n",
    "dataset = H2RDataset(**new_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[160.6198, 344.0589, 123.0000,  -0.4481,   0.0000,   0.8940,  -0.7992,\n",
       "          -0.4481,  -0.4006,   0.0000],\n",
       "        [158.3165, 344.8860, 123.0000,  -0.4481,   0.0000,   0.8940,  -0.7992,\n",
       "          -0.4481,  -0.4006,   0.0000],\n",
       "        [158.3165, 344.8860, 123.0000,  -0.4481,   0.0000,   0.8940,  -0.7992,\n",
       "          -0.4481,  -0.4006,   0.0000],\n",
       "        [158.3165, 344.8860, 123.0000,  -0.4481,   0.0000,   0.8940,  -0.7992,\n",
       "          -0.4481,  -0.4006,   0.0000]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[8]['action']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[160.6198, 344.0589, 123.0000,  -0.4481,   0.0000,   0.8940,  -0.7992,\n",
       "          -0.4481,  -0.4006,   0.0000],\n",
       "        [160.6198, 344.0589, 123.0000,  -0.4481,   0.0000,   0.8940,  -0.7992,\n",
       "          -0.4481,  -0.4006,   0.0000],\n",
       "        [158.3165, 344.8860, 123.0000,  -0.4481,   0.0000,   0.8940,  -0.7992,\n",
       "          -0.4481,  -0.4006,   0.0000],\n",
       "        [158.3165, 344.8860, 123.0000,  -0.4481,   0.0000,   0.8940,  -0.7992,\n",
       "          -0.4481,  -0.4006,   0.0000]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[7][\"action\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[160.6198, 344.0589, 123.0000,  -0.4481,   0.0000,   0.8940,  -0.7992,\n",
       "          -0.4481,  -0.4006,   0.0000],\n",
       "        [160.6198, 344.0589, 123.0000,  -0.4481,   0.0000,   0.8940,  -0.7992,\n",
       "          -0.4481,  -0.4006,   0.0000],\n",
       "        [160.6198, 344.0589, 123.0000,  -0.4481,   0.0000,   0.8940,  -0.7992,\n",
       "          -0.4481,  -0.4006,   0.0000],\n",
       "        [158.3165, 344.8860, 123.0000,  -0.4481,   0.0000,   0.8940,  -0.7992,\n",
       "          -0.4481,  -0.4006,   0.0000]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[6][\"action\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robodiff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
